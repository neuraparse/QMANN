{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QMNN Basic Tutorial\n",
    "\n",
    "This notebook demonstrates the basic usage of Quantum Memory-Augmented Neural Networks (QMNN).\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand QMNN architecture\n",
    "- Create and train a basic QMNN model\n",
    "- Explore quantum memory operations\n",
    "- Compare with classical baselines\n",
    "- Visualize quantum advantages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install QMNN if not already installed\n",
    "# !pip install -e .\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# QMNN imports\n",
    "from qmnn import QMNN, QuantumMemory, QMNNTrainer\n",
    "from qmnn import get_system_info, get_available_features\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"QMNN Tutorial - Basic Usage\")\n",
    "print(\"=\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. System Information and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check system capabilities\n",
    "system_info = get_system_info()\n",
    "features = get_available_features()\n",
    "\n",
    "print(\"System Information:\")\n",
    "print(f\"- QMNN Version: {system_info['qmnn_version']}\")\n",
    "print(f\"- PyTorch Version: {system_info['pytorch_version']}\")\n",
    "print(f\"- CUDA Available: {system_info['cuda_available']}\")\n",
    "print(f\"- Max Recommended Qubits: {system_info['recommended_config']['max_qubits']}\")\n",
    "\n",
    "print(\"\\nAvailable Features:\")\n",
    "for feature, available in features.items():\n",
    "    status = \"✓\" if available else \"✗\"\n",
    "    print(f\"- {feature}: {status}\")\n",
    "\n",
    "# Set device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"\\nUsing device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding Quantum Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a quantum memory instance\n",
    "memory = QuantumMemory(\n",
    "    capacity=32,\n",
    "    embedding_dim=16,\n",
    "    max_qubits=8\n",
    ")\n",
    "\n",
    "print(\"Quantum Memory Created:\")\n",
    "print(f\"- Capacity: {memory.effective_capacity}\")\n",
    "print(f\"- Embedding Dimension: {memory.embedding_dim}\")\n",
    "print(f\"- Address Qubits: {memory.qram.address_qubits}\")\n",
    "print(f\"- Data Qubits: {memory.qram.max_data_qubits}\")\n",
    "\n",
    "# Store some example data\n",
    "print(\"\\nStoring example data...\")\n",
    "for i in range(5):\n",
    "    key = np.random.randn(16)\n",
    "    value = np.random.randn(16)\n",
    "    address = memory.store_embedding(key, value)\n",
    "    print(f\"Stored item {i+1} at address {address}\")\n",
    "\n",
    "# Check memory usage\n",
    "usage = memory.memory_usage()\n",
    "print(f\"\\nMemory Usage: {usage:.2%}\")\n",
    "\n",
    "# Test retrieval\n",
    "query = key + 0.1 * np.random.randn(16)  # Noisy version of last key\n",
    "retrieved = memory.retrieve_embedding(query)\n",
    "similarity = np.dot(value, retrieved) / (np.linalg.norm(value) * np.linalg.norm(retrieved))\n",
    "print(f\"Retrieval similarity: {similarity:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creating a QMNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model parameters\n",
    "config = {\n",
    "    'input_dim': 10,\n",
    "    'hidden_dim': 64,\n",
    "    'output_dim': 3,\n",
    "    'memory_capacity': 32,\n",
    "    'memory_embedding_dim': 32,\n",
    "    'n_quantum_layers': 2,\n",
    "    'max_qubits': 8,\n",
    "    'use_attention': True,\n",
    "    'dropout': 0.1\n",
    "}\n",
    "\n",
    "# Create QMNN model\n",
    "model = QMNN(**config).to(device)\n",
    "\n",
    "print(\"QMNN Model Created:\")\n",
    "print(f\"- Total Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Get quantum information\n",
    "quantum_info = model.get_quantum_info()\n",
    "print(f\"- Quantum Parameters: {quantum_info['quantum_parameters']:,}\")\n",
    "print(f\"- Classical Parameters: {quantum_info['classical_parameters']:,}\")\n",
    "print(f\"- Quantum Ratio: {quantum_info['quantum_ratio']:.2%}\")\n",
    "print(f\"- Memory Capacity: {quantum_info['memory_capacity']}\")\n",
    "\n",
    "print(\"\\nModel Architecture:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generating Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_memory_task_data(n_samples=1000, seq_len=15, input_dim=10, n_classes=3):\n",
    "    \"\"\"\n",
    "    Generate data for a memory-dependent classification task.\n",
    "    The model needs to remember information from early in the sequence\n",
    "    to classify the entire sequence correctly.\n",
    "    \"\"\"\n",
    "    X = torch.randn(n_samples, seq_len, input_dim)\n",
    "    y = torch.zeros(n_samples, seq_len, dtype=torch.long)\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # Create memory cue in first few positions\n",
    "        memory_cue = torch.randint(0, n_classes, (1,)).item()\n",
    "        X[i, 0, 0] = memory_cue  # Embed cue in first position\n",
    "        \n",
    "        # Classification depends on memory cue and current input\n",
    "        for t in range(seq_len):\n",
    "            if t < 3:  # Early positions\n",
    "                y[i, t] = memory_cue\n",
    "            else:  # Later positions depend on memory + current input\n",
    "                current_signal = (X[i, t, 1] > 0).long().item()\n",
    "                y[i, t] = (memory_cue + current_signal) % n_classes\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Generate training and test data\n",
    "print(\"Generating memory task data...\")\n",
    "X_train, y_train = generate_memory_task_data(800, 15, config['input_dim'], config['output_dim'])\n",
    "X_test, y_test = generate_memory_task_data(200, 15, config['input_dim'], config['output_dim'])\n",
    "\n",
    "X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "\n",
    "print(f\"Training data: {X_train.shape}, {y_train.shape}\")\n",
    "print(f\"Test data: {X_test.shape}, {y_test.shape}\")\n",
    "\n",
    "# Visualize a sample\n",
    "plt.figure(figsize=(12, 4))\n",
    "sample_idx = 0\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(X_train[sample_idx].cpu().T, aspect='auto', cmap='viridis')\n",
    "plt.title('Input Sequence (Features over Time)')\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Feature Dimension')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(y_train[sample_idx].cpu(), 'o-')\n",
    "plt.title('Target Sequence')\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Class')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training the QMNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer\n",
    "trainer = QMNNTrainer(\n",
    "    model=model,\n",
    "    learning_rate=1e-3,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "n_epochs = 20\n",
    "batch_size = 32\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "memory_usages = []\n",
    "\n",
    "print(\"Training QMNN model...\")\n",
    "for epoch in tqdm(range(n_epochs)):\n",
    "    # Training\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    n_batches = 0\n",
    "    \n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        batch_X = X_train[i:i+batch_size]\n",
    "        batch_y = y_train[i:i+batch_size]\n",
    "        \n",
    "        metrics = trainer.train_step(batch_X, batch_y)\n",
    "        epoch_loss += metrics['loss']\n",
    "        epoch_acc += metrics['accuracy']\n",
    "        n_batches += 1\n",
    "    \n",
    "    train_losses.append(epoch_loss / n_batches)\n",
    "    train_accuracies.append(epoch_acc / n_batches)\n",
    "    \n",
    "    # Validation\n",
    "    test_metrics = trainer.validate(X_test, y_test)\n",
    "    test_accuracies.append(test_metrics['accuracy'])\n",
    "    \n",
    "    # Memory usage\n",
    "    memory_usage = model.quantum_memory.memory_usage()\n",
    "    memory_usages.append(memory_usage)\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs}:\")\n",
    "        print(f\"  Train Loss: {train_losses[-1]:.4f}\")\n",
    "        print(f\"  Train Acc: {train_accuracies[-1]:.4f}\")\n",
    "        print(f\"  Test Acc: {test_accuracies[-1]:.4f}\")\n",
    "        print(f\"  Memory Usage: {memory_usages[-1]:.2%}\")\n",
    "\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analyzing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(train_accuracies, label='Training Accuracy')\n",
    "plt.plot(test_accuracies, label='Test Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(memory_usages, label='Memory Usage')\n",
    "plt.title('Quantum Memory Usage')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Usage Ratio')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final Test Accuracy: {test_accuracies[-1]:.4f}\")\n",
    "print(f\"Final Memory Usage: {memory_usages[-1]:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Quantum Memory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze quantum memory behavior\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Get model predictions and memory info\n",
    "    sample_batch = X_test[:5]\n",
    "    predictions, memory_info = model(sample_batch)\n",
    "    \n",
    "    print(\"Memory Analysis:\")\n",
    "    print(f\"- Memory reads: {memory_info.get('memory_reads', 'N/A')}\")\n",
    "    print(f\"- Memory writes: {memory_info.get('memory_writes', 'N/A')}\")\n",
    "    print(f\"- Memory usage: {memory_info['memory_usage']:.2%}\")\n",
    "    \n",
    "    # Get quantum information\n",
    "    quantum_info = model.get_quantum_info()\n",
    "    print(f\"\\nQuantum Components:\")\n",
    "    print(f\"- Quantum layers: {quantum_info['n_quantum_layers']}\")\n",
    "    print(f\"- Quantum parameters: {quantum_info['quantum_parameters']:,}\")\n",
    "    print(f\"- Classical parameters: {quantum_info['classical_parameters']:,}\")\n",
    "    print(f\"- Quantum advantage ratio: {quantum_info['quantum_ratio']:.2%}\")\n",
    "\n",
    "# Memory capacity analysis\n",
    "capacity_info = model.quantum_memory.capacity_bound()\n",
    "print(f\"\\nMemory Capacity Analysis:\")\n",
    "for key, value in capacity_info.items():\n",
    "    print(f\"- {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Comparison with Classical Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a classical LSTM baseline\n",
    "class LSTMBaseline(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.lstm = torch.nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.classifier = torch.nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        return self.classifier(lstm_out)\n",
    "\n",
    "# Train LSTM baseline\n",
    "lstm_model = LSTMBaseline(config['input_dim'], config['hidden_dim'], config['output_dim']).to(device)\n",
    "lstm_trainer = QMNNTrainer(lstm_model, learning_rate=1e-3, device=device)\n",
    "\n",
    "print(\"Training LSTM baseline...\")\n",
    "lstm_accuracies = []\n",
    "\n",
    "for epoch in tqdm(range(n_epochs)):\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        batch_X = X_train[i:i+batch_size]\n",
    "        batch_y = y_train[i:i+batch_size]\n",
    "        lstm_trainer.train_step(batch_X, batch_y)\n",
    "    \n",
    "    test_metrics = lstm_trainer.validate(X_test, y_test)\n",
    "    lstm_accuracies.append(test_metrics['accuracy'])\n",
    "\n",
    "# Compare results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(test_accuracies, label='QMNN', linewidth=2)\n",
    "plt.plot(lstm_accuracies, label='LSTM Baseline', linewidth=2)\n",
    "plt.title('QMNN vs Classical Baseline')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal Results:\")\n",
    "print(f\"QMNN Test Accuracy: {test_accuracies[-1]:.4f}\")\n",
    "print(f\"LSTM Test Accuracy: {lstm_accuracies[-1]:.4f}\")\n",
    "improvement = (test_accuracies[-1] - lstm_accuracies[-1]) / lstm_accuracies[-1] * 100\n",
    "print(f\"QMNN Improvement: {improvement:+.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusions\n",
    "\n",
    "In this tutorial, we:\n",
    "\n",
    "1. **Explored quantum memory**: Learned how QRAM stores and retrieves information using quantum superposition\n",
    "2. **Built a QMNN model**: Created a hybrid quantum-classical neural network\n",
    "3. **Trained on memory tasks**: Demonstrated QMNN's ability to handle memory-dependent problems\n",
    "4. **Analyzed quantum components**: Examined the quantum vs classical parameter ratio\n",
    "5. **Compared with baselines**: Showed potential quantum advantages\n",
    "\n",
    "### Key Takeaways:\n",
    "- QMNN combines classical neural networks with quantum memory for enhanced capacity\n",
    "- Quantum memory can provide advantages for tasks requiring long-term memory\n",
    "- The system automatically handles hardware constraints and optimization\n",
    "- Performance depends on the specific task and quantum component configuration\n",
    "\n",
    "### Next Steps:\n",
    "- Try different memory tasks and configurations\n",
    "- Explore quantum transformers and attention mechanisms\n",
    "- Experiment with error correction and federated learning\n",
    "- Test on real quantum hardware (when available)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
