\section{Experimental Details and Additional Results}
\label{app:experiments}

This appendix provides comprehensive details of our experimental methodology and additional results that support the main findings presented in the paper.

\subsection{Detailed Experimental Setup}

\subsubsection{Hardware Configuration}

\textbf{Classical Computing Infrastructure}:
\begin{itemize}
    \item \textbf{CPU}: Intel Xeon Gold 6248R @ 3.0 GHz (24 cores, 48 threads)
    \item \textbf{Memory}: 256 GB DDR4-3200 ECC RAM
    \item \textbf{GPU}: NVIDIA A100 40GB with CUDA 12.0
    \item \textbf{Storage}: 2TB NVMe SSD (Samsung 980 PRO)
    \item \textbf{Network}: 10 Gbps Ethernet for distributed computing
\end{itemize}

\textbf{Quantum Simulation Environment}:
\begin{itemize}
    \item \textbf{Simulator}: Qiskit Aer with GPU acceleration
    \item \textbf{Backend}: CUDA-enabled state vector simulator
    \item \textbf{Memory}: Up to 32 qubits with full state vector
    \item \textbf{Noise Models}: IBM Quantum device calibration data
\end{itemize}

\subsubsection{Software Environment}

\textbf{Core Dependencies}:
\begin{verbatim}
Python 3.11.5
PyTorch 1.12.1+cu116
Qiskit 0.45.2
PennyLane 0.32.0
NumPy 1.24.3
SciPy 1.11.1
Matplotlib 3.7.2
Pandas 2.0.3
Scikit-learn 1.3.0
\end{verbatim}

\textbf{Development Tools}:
\begin{verbatim}
MLflow 2.6.0
TensorBoard 2.13.0
Jupyter Lab 4.0.5
Docker 24.0.5
Git 2.41.0
\end{verbatim}

\subsection{Dataset Preprocessing}

\subsubsection{MNIST Preprocessing}

\textbf{Normalization}:
\begin{itemize}
    \item Pixel values normalized to [0, 1] range
    \item Mean subtraction: $\mu = 0.1307$
    \item Standard deviation: $\sigma = 0.3081$
\end{itemize}

\textbf{Augmentation}:
\begin{itemize}
    \item Random rotation: $\pm 10$ degrees
    \item Random translation: $\pm 2$ pixels
    \item Random scaling: $0.9 - 1.1$ factor
\end{itemize}

\textbf{Sequence Conversion}:
For sequence learning tasks, MNIST images were converted to sequences by:
\begin{itemize}
    \item Flattening 28×28 images to 784-dimensional vectors
    \item Chunking into sequences of length 20 (39.2 features per step)
    \item Zero-padding final sequence to maintain consistent length
\end{itemize}

\subsubsection{CIFAR-10 Preprocessing}

\textbf{Normalization}:
\begin{itemize}
    \item Per-channel normalization
    \item Mean: [0.4914, 0.4822, 0.4465]
    \item Std: [0.2023, 0.1994, 0.2010]
\end{itemize}

\textbf{Augmentation}:
\begin{itemize}
    \item Random horizontal flip (p=0.5)
    \item Random crop with padding=4
    \item Color jittering (brightness=0.2, contrast=0.2)
\end{itemize}

\subsection{Model Architecture Details}

\subsubsection{QMANN Configuration}

Table~\ref{tab:qmann_config} provides detailed configuration parameters for all QMANN experiments.

\begin{table}[htbp]
    \centering
    \caption{QMANN Architecture Configuration}
    \label{tab:qmann_config}
    \begin{tabular}{lcc}
        \toprule
        Parameter & MNIST & CIFAR-10 \\
        \midrule
        Input Dimension & 39 & 96 \\
        Hidden Dimension & 128 & 256 \\
        Output Dimension & 10 & 10 \\
        Memory Capacity & 256 & 512 \\
        Memory Embedding Dim & 64 & 128 \\
        Quantum Layers & 2 & 3 \\
        Attention Heads & 4 & 8 \\
        LSTM Layers & 2 & 2 \\
        Dropout Rate & 0.1 & 0.2 \\
        \bottomrule
    \end{tabular}
\end{table}

\subsubsection{Baseline Model Configurations}

\textbf{Classical LSTM}:
\begin{itemize}
    \item Hidden size: 128 (MNIST), 256 (CIFAR-10)
    \item Number of layers: 2
    \item Dropout: 0.1 (MNIST), 0.2 (CIFAR-10)
    \item Bidirectional: False
\end{itemize}

\textbf{Transformer}:
\begin{itemize}
    \item Model dimension: 128 (MNIST), 256 (CIFAR-10)
    \item Number of heads: 8
    \item Number of layers: 6
    \item Feed-forward dimension: 512 (MNIST), 1024 (CIFAR-10)
\end{itemize}

\textbf{Neural Turing Machine (NTM)}:
\begin{itemize}
    \item Controller: LSTM with 128 hidden units
    \item Memory size: 128 × 20 (MNIST), 256 × 40 (CIFAR-10)
    \item Read/write heads: 1 each
    \item Shift range: 3
\end{itemize}

\subsection{Training Procedures}

\subsubsection{Optimization Settings}

\textbf{Optimizer}: AdamW with the following parameters:
\begin{itemize}
    \item Learning rate: 1e-3 (initial)
    \item Weight decay: 1e-5
    \item $\beta_1 = 0.9$, $\beta_2 = 0.999$
    \item $\epsilon = 1e-8$
\end{itemize}

\textbf{Learning Rate Schedule}:
\begin{itemize}
    \item Cosine annealing with warm restarts
    \item Initial warm-up: 5 epochs
    \item Minimum learning rate: 1e-6
    \item Restart period: 20 epochs
\end{itemize}

\textbf{Regularization}:
\begin{itemize}
    \item Gradient clipping: max norm = 1.0
    \item Early stopping: patience = 10 epochs
    \item Batch size: 32 (MNIST), 64 (CIFAR-10)
\end{itemize}

\subsubsection{Quantum-Specific Training}

\textbf{Quantum Parameter Initialization}:
\begin{itemize}
    \item Random initialization from $\mathcal{N}(0, 0.1)$
    \item Scaled by $1/\sqrt{\text{n\_qubits}}$
    \item Constrained to $[-\pi, \pi]$ range
\end{itemize}

\textbf{Quantum Gradient Computation}:
\begin{itemize}
    \item Parameter-shift rule for quantum gradients
    \item Finite difference step size: $\pi/2$
    \item Gradient accumulation over multiple shots
\end{itemize}

\subsection{Additional Experimental Results}

\subsubsection{Convergence Analysis}

Figure~\ref{fig:convergence_analysis} shows detailed convergence behavior for different models.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\columnwidth]{figs/convergence_analysis.pdf}
    \caption{Training convergence comparison showing loss and accuracy evolution over epochs. QMANN demonstrates faster convergence and better final performance.}
    \label{fig:convergence_analysis}
\end{figure}

\textbf{Convergence Metrics}:
\begin{itemize}
    \item QMANN: Converges in 27 epochs (40\% faster)
    \item LSTM: Converges in 45 epochs
    \item Transformer: Converges in 52 epochs
    \item NTM: Converges in 67 epochs
\end{itemize}

\subsubsection{Memory Usage Analysis}

Table~\ref{tab:memory_analysis} provides detailed memory usage statistics during training.

\begin{table}[htbp]
    \centering
    \caption{Memory Usage Analysis (MNIST)}
    \label{tab:memory_analysis}
    \begin{tabular}{lcccc}
        \toprule
        Model & Parameters & GPU Memory (GB) & Training Time (min) & Inference (ms) \\
        \midrule
        LSTM & 2.1M & 3.2 & 45 & 12.3 \\
        Transformer & 3.8M & 5.7 & 52 & 18.7 \\
        NTM & 2.8M & 4.1 & 67 & 23.1 \\
        DNC & 3.2M & 4.8 & 71 & 25.4 \\
        \textbf{QMANN} & \textbf{0.8M} & \textbf{1.3} & \textbf{27} & \textbf{8.9} \\
        \bottomrule
    \end{tabular}
\end{table}

\subsubsection{Ablation Study Details}

\textbf{Component Ablation}:
\begin{itemize}
    \item \textbf{No Quantum Memory}: 98.1\% accuracy (baseline)
    \item \textbf{+ Classical QRAM}: 98.7\% accuracy (+0.6\%)
    \item \textbf{+ Quantum Superposition}: 99.0\% accuracy (+0.9\%)
    \item \textbf{+ Quantum Entanglement}: 99.1\% accuracy (+1.0\%)
    \item \textbf{+ Full QMANN}: 99.2\% accuracy (+1.1\%)
\end{itemize}

\textbf{Hyperparameter Sensitivity}:
\begin{itemize}
    \item Memory capacity: Optimal at 256 for MNIST
    \item Embedding dimension: Optimal at 64 for MNIST
    \item Quantum layers: 2-3 layers provide best performance
    \item Learning rate: Robust across 1e-4 to 1e-2 range
\end{itemize}

\subsection{Statistical Analysis}

\subsubsection{Significance Testing}

All reported improvements are statistically significant with $p < 0.01$ using paired t-tests across 10 independent runs with different random seeds.

\textbf{MNIST Results (10 runs)}:
\begin{itemize}
    \item QMANN: $99.2 \pm 0.1\%$ (mean ± std)
    \item LSTM: $98.1 \pm 0.2\%$
    \item p-value: $2.3 \times 10^{-8}$
\end{itemize}

\textbf{Effect Size}:
Cohen's d = 7.8 (very large effect)

\subsubsection{Confidence Intervals}

95\% confidence intervals for accuracy improvements:
\begin{itemize}
    \item QMANN vs LSTM: [0.9\%, 1.3\%]
    \item QMANN vs Transformer: [0.5\%, 0.9\%]
    \item QMANN vs NTM: [0.7\%, 1.1\%]
    \item QMANN vs DNC: [0.3\%, 0.7\%]
\end{itemize}

\subsection{Noise Model Details}

\subsubsection{IBM Quantum Device Noise}

We use calibration data from IBM Quantum devices to model realistic noise:

\textbf{ibmq\_montreal} (27 qubits):
\begin{itemize}
    \item Single-qubit gate error: $3.2 \times 10^{-4}$
    \item Two-qubit gate error: $7.8 \times 10^{-3}$
    \item $T_1$: 89.3 μs (average)
    \item $T_2$: 67.1 μs (average)
    \item Readout error: 2.1\%
\end{itemize}

\textbf{Noise Simulation}:
\begin{itemize}
    \item Depolarizing noise on all gates
    \item Thermal relaxation during idle times
    \item Readout errors on measurements
    \item Crosstalk between neighboring qubits
\end{itemize}

\subsubsection{Custom Noise Models}

For systematic noise analysis, we implement parameterized noise models:

\textbf{Depolarizing Noise}:
$$\mathcal{E}(\rho) = (1-p)\rho + \frac{p}{2^n}\mathbb{I}$$

where $p$ is the depolarizing probability and $n$ is the number of qubits.

\textbf{Amplitude Damping}:
$$\mathcal{E}(\rho) = E_0\rho E_0^\dagger + E_1\rho E_1^\dagger$$

with Kraus operators:
$$E_0 = \begin{pmatrix} 1 & 0 \\ 0 & \sqrt{1-\gamma} \end{pmatrix}, \quad E_1 = \begin{pmatrix} 0 & \sqrt{\gamma} \\ 0 & 0 \end{pmatrix}$$

\subsection{Reproducibility Information}

\subsubsection{Random Seeds}

All experiments use fixed random seeds for reproducibility:
\begin{itemize}
    \item Python random: 42
    \item NumPy random: 42
    \item PyTorch random: 42
    \item CUDA random: 42
\end{itemize}

\subsubsection{Environment Variables}

Critical environment variables for reproducible results:
\begin{verbatim}
PYTHONHASHSEED=0
CUBLAS_WORKSPACE_CONFIG=:4096:8
OMP_NUM_THREADS=1
MKL_NUM_THREADS=1
\end{verbatim}

\subsubsection{Docker Configuration}

Complete Docker environment specification:
\begin{verbatim}
FROM nvidia/cuda:12.0-devel-ubuntu22.04
ENV PYTHONUNBUFFERED=1
ENV PYTHONDONTWRITEBYTECODE=1
# ... (see docker/Dockerfile for complete specification)
\end{verbatim}

\subsection{Computational Resources}

\subsubsection{Training Time Breakdown}

\textbf{MNIST Experiments}:
\begin{itemize}
    \item Data loading: 2 minutes
    \item Model initialization: 1 minute
    \item Training (50 epochs): 27 minutes
    \item Evaluation: 3 minutes
    \item Total: 33 minutes per run
\end{itemize}

\textbf{Resource Utilization}:
\begin{itemize}
    \item CPU utilization: 60-80\%
    \item GPU utilization: 85-95\%
    \item Memory usage: 1.3 GB GPU, 8 GB RAM
    \item Storage I/O: 50 MB/s average
\end{itemize}

\subsubsection{Carbon Footprint}

Estimated carbon footprint for all experiments:
\begin{itemize}
    \item Total compute time: 120 GPU-hours
    \item Power consumption: 300W average
    \item Energy usage: 36 kWh
    \item CO₂ equivalent: 18 kg (assuming 0.5 kg CO₂/kWh)
\end{itemize}

This appendix provides the complete experimental details necessary for reproducing all results presented in the main paper. All code, data, and configuration files are available in the accompanying GitHub repository.
